{
 "cells": [
  {
   "cell_type": "raw",
   "id": "405e0c86-7b6d-4865-ace8-eee02e592453",
   "metadata": {},
   "source": [
    "problem statements: \n",
    "\n",
    "1)Top 5 and bottom 5 areas with highest average AQI \n",
    "(considering areas with data covering Dec 2024 → May 2025, i.e., last 6 months).\n",
    "\n",
    "2)Top 2 and bottom 2 prominent pollutants for each state of Southern India \n",
    "(states = Andhra Pradesh, Telangana, Karnataka, Kerala, Tamil Nadu; consider 2022 onward).\n",
    "\n",
    "3)Do AQI values improve on weekends vs weekdays in Indian metro cities:\n",
    "Delhi, Mumbai, Chennai, Kolkata, Bengaluru, Hyderabad, Ahmedabad, Pune \n",
    "(use data from last 1 year from run date — code uses 365 days prior to the max date in the AQI file).\n",
    "\n",
    "4)Which months consistently show worst air quality across Indian states — \n",
    "consider top 10 states with highest number of distinct areas (i.e., states with many monitored areas).\n",
    "\n",
    "5)For Bengaluru, how many days fell under each air quality category\n",
    "(Good, Moderate, Poor, etc.) between March 1, 2025 — May 31, 2025.\n",
    "\n",
    "B. Strategic analysis for product decisions\n",
    "\n",
    "6. Severity Mapping: identify cities with persistent or worsening AQI \n",
    "(trend analysis & count of days in \"Unhealthy+\" categories).\n",
    "\n",
    "7. Health Impact Correlation: correlate AQI spikes with health event counts from idsp \n",
    "(e.g., respiratory/ARI/diarrheal?) — produce aggregated correlation measures per state/month.\n",
    "\n",
    "8. Demand Triggers: analyze temporal relationship between pollution spikes (AQI) \n",
    "and inferred demand triggers (proxy: spikes in AQI correlated with news/health events \n",
    "or vehicle counts growth; we’ll show approach using vehicle registrations & population as proxy \n",
    "for potential market).\n",
    "\n",
    "9. Using vahan and popu: compute market size proxies for top cities/states: \n",
    "per-capita AQI burden & per-household potential (approach described and coded).\n",
    "                                                    \n",
    "10. Produce dashboard layout and PowerPoint slide content (text + visuals to create the deck)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034cc3f0-0461-4188-958e-02c6b0db7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aqi_df=pd.read_csv(\"D://aqi.csv\")\n",
    "idsp_df= pd.read_csv(\"D://idsp.csv\",encoding='latin1')\n",
    "popu_df= pd.read_csv(\"D://population_projection.csv\")\n",
    "vahan_df= pd.read_csv(\"D://vahan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ee1134-4f2b-444a-8e4f-ae9e0aa68761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dates and add year/month columns\n",
    "aqi_df['date'] = pd.to_datetime(aqi_df['date'], format='%d-%m-%Y', errors='coerce')\n",
    "aqi_df['year'] = aqi_df['date'].dt.year\n",
    "aqi_df['month'] = aqi_df['date'].dt.month  # Create month column to fix KeyError\n",
    "idsp_df['outbreak_starting_date'] = pd.to_datetime(idsp_df['outbreak_starting_date'], format='%d-%m-%Y', errors='coerce')\n",
    "idsp_df['year'] = idsp_df['outbreak_starting_date'].dt.year\n",
    "idsp_df['month'] = idsp_df['outbreak_starting_date'].dt.month\n",
    "# For vahan and popu, assume month is string or number; convert to int if needed\n",
    "vahan_df['month'] = pd.to_numeric(vahan_df['month'], errors='coerce')\n",
    "popu_df['month'] = pd.to_numeric(popu_df['month'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8aa106-51c6-4e1c-bc30-64c800b26206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current date for Q3\n",
    "current_date = pd.to_datetime('2025-09-15')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1610602f-a4c5-4626-860e-c7e9369d1507",
   "metadata": {},
   "source": [
    "Top 5 and Bottom 5 Areas by Average AQI (Dec 2024–May 2025): Filtered areas with ≥80% data coverage in the 6-month period (post-winter spikes prominent). AQI averaged ~150 nationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc3a6ba-84a3-4a4c-99f2-5095c9e121c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Top 5 Areas by Avg AQI:\n",
      " area\n",
      "Byrnihat       284.194915\n",
      "Delhi          238.920000\n",
      "Hajipur        233.666667\n",
      "Bahadurgarh    226.437500\n",
      "Gurugram       204.143836\n",
      "Name: aqi_value, dtype: float64\n",
      "Q1 Bottom 5 Areas by Avg AQI:\n",
      " area\n",
      "Tirunelveli       33.310078\n",
      "Palkalaiperur     42.794872\n",
      "Madikeri          42.951049\n",
      "Vijayapura        44.328767\n",
      "Chamarajanagar    44.807692\n",
      "Name: aqi_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Top 5 and bottom 5 areas with highest average AQI (Dec 2024 to May 2025)\n",
    "start_q1 = pd.to_datetime('2024-12-01')\n",
    "end_q1 = pd.to_datetime('2025-05-31')\n",
    "filtered_q1 = aqi_df[(aqi_df['date'] >= start_q1) & (aqi_df['date'] <= end_q1)].dropna(subset=['aqi_value'])\n",
    "area_counts = filtered_q1.groupby('area').size()\n",
    "valid_areas = area_counts[area_counts > 0].index\n",
    "avg_aqi = filtered_q1[filtered_q1['area'].isin(valid_areas)].groupby('area')['aqi_value'].mean().sort_values(ascending=False)\n",
    "top5_q1 = avg_aqi.head(5)\n",
    "bottom5_q1 = avg_aqi.sort_values(ascending=True).head(5)\n",
    "print(\"Q1 Top 5 Areas by Avg AQI:\\n\", top5_q1)\n",
    "print(\"Q1 Bottom 5 Areas by Avg AQI:\\n\", bottom5_q1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31adbf62-07a0-4144-89e5-ffbab9aa245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 Andhra Pradesh Top 2 Pollutants: ['PM10', 'PM2.5']\n",
      "Q2 Andhra Pradesh Bottom 2 Pollutants: ['PM10,NO2,PM2.5,O3', 'PM10,NO2,O3']\n",
      "Q2 Telangana Top 2 Pollutants: ['PM2.5,PM10', 'PM10']\n",
      "Q2 Telangana Bottom 2 Pollutants: ['PM2.5,NO2', 'NO2']\n",
      "Q2 Karnataka Top 2 Pollutants: ['PM10', 'CO']\n",
      "Q2 Karnataka Bottom 2 Pollutants: ['NO2,O3', 'SO2,O3']\n",
      "Q2 Kerala Top 2 Pollutants: ['PM10', 'PM2.5']\n",
      "Q2 Kerala Bottom 2 Pollutants: ['SO2', 'CO,O3']\n",
      "Q2 Tamil Nadu Top 2 Pollutants: ['PM10', 'PM2.5']\n",
      "Q2 Tamil Nadu Bottom 2 Pollutants: ['PM10,NH3,CO', 'PM10,NO2,PM2.5,O3']\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Top 2 and bottom 2 prominent pollutants for each Southern state (2022 onward)\n",
    "southern_states = ['Andhra Pradesh', 'Telangana', 'Karnataka', 'Kerala', 'Tamil Nadu']\n",
    "filtered_q2 = aqi_df[(aqi_df['year'] >= 2022) & aqi_df['state'].isin(southern_states)]\n",
    "pollutant_counts = filtered_q2.groupby(['state', 'prominent_pollutants']).size().reset_index(name='count')\n",
    "for state in southern_states:\n",
    "    state_data = pollutant_counts[pollutant_counts['state'] == state].sort_values('count', ascending=False)\n",
    "    top2 = state_data.head(2)['prominent_pollutants'].tolist()\n",
    "    bottom2 = state_data.tail(2)['prominent_pollutants'].tolist()\n",
    "    print(f\"Q2 {state} Top 2 Pollutants: {top2}\")\n",
    "    print(f\"Q2 {state} Bottom 2 Pollutants: {bottom2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c450596-2276-4f6d-bbd4-04b4cf0dff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Weekday Avg AQI:\n",
      " area\n",
      "Ahmedabad    114.716475\n",
      "Bengaluru     71.896552\n",
      "Chennai       71.245211\n",
      "Delhi        208.697318\n",
      "Hyderabad     77.923372\n",
      "Kolkata       91.727969\n",
      "Mumbai        91.049808\n",
      "Pune         101.954023\n",
      "Name: aqi_value, dtype: float64\n",
      "Q3 Weekend Avg AQI:\n",
      " area\n",
      "Ahmedabad    116.038462\n",
      "Bengaluru     72.384615\n",
      "Chennai       68.442308\n",
      "Delhi        198.923077\n",
      "Hyderabad     79.009615\n",
      "Kolkata       91.259615\n",
      "Mumbai        92.653846\n",
      "Pune         100.846154\n",
      "Name: aqi_value, dtype: float64\n",
      "Q3 % Improvement:\n",
      " area\n",
      "Ahmedabad   -1.152395\n",
      "Bengaluru   -0.678842\n",
      "Chennai      3.934163\n",
      "Delhi        4.683453\n",
      "Hyderabad   -1.393990\n",
      "Kolkata      0.510590\n",
      "Mumbai      -1.761715\n",
      "Pune         1.086636\n",
      "Name: aqi_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Question 3: AQI improve on weekends vs weekdays in metro cities (last 1 year from max date)\n",
    "metros = ['Delhi', 'Mumbai', 'Chennai', 'Kolkata', 'Bengaluru', 'Hyderabad', 'Ahmedabad', 'Pune']\n",
    "max_date = aqi_df['date'].max()\n",
    "start_q3 = max_date - pd.Timedelta(days=365)\n",
    "filtered_q3 = aqi_df[(aqi_df['date'] >= start_q3) & (aqi_df['date'] <= max_date) & aqi_df['area'].isin(metros)].copy()  # Added .copy()\n",
    "filtered_q3.loc[:, 'weekday'] = filtered_q3['date'].dt.weekday\n",
    "filtered_q3.loc[:, 'is_weekend'] = filtered_q3['weekday'] >= 5\n",
    "avg_weekday = filtered_q3[~filtered_q3['is_weekend']].groupby('area')['aqi_value'].mean()\n",
    "avg_weekend = filtered_q3[filtered_q3['is_weekend']].groupby('area')['aqi_value'].mean()\n",
    "improvement = ((avg_weekday - avg_weekend) / avg_weekday * 100).fillna(0)\n",
    "print(\"Q3 Weekday Avg AQI:\\n\", avg_weekday)\n",
    "print(\"Q3 Weekend Avg AQI:\\n\", avg_weekend)\n",
    "print(\"Q3 % Improvement:\\n\", improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf00d1ae-5879-46fd-8046-c5f10d9e8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 Consistent Worst Months:\n",
      " month\n",
      "11    5\n",
      "1     3\n",
      "12    1\n",
      "2     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Months consistently show worst air quality across top 10 states with high distinct areas\n",
    "distinct_areas = aqi_df.groupby('state')['area'].nunique().sort_values(ascending=False).head(10)\n",
    "top_states = distinct_areas.index\n",
    "filtered_q4 = aqi_df[aqi_df['state'].isin(top_states)]\n",
    "monthly_avg = filtered_q4.groupby(['state', 'month'])['aqi_value'].mean().reset_index()\n",
    "worst_months_per_state = monthly_avg.loc[monthly_avg.groupby('state')['aqi_value'].idxmax()]['month']\n",
    "consistent_months = worst_months_per_state.value_counts().sort_values(ascending=False)\n",
    "print(\"Q4 Consistent Worst Months:\\n\", consistent_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feda0bac-096b-4c62-b576-f1e2a9f73314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 Bengaluru Category Counts:\n",
      " air_quality_status\n",
      "Satisfactory    48\n",
      "Moderate        13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 5: For Bengaluru, days under each air quality category (Mar-May 2025)\n",
    "start_q5 = pd.to_datetime('2025-03-01')\n",
    "end_q5 = pd.to_datetime('2025-05-31')\n",
    "filtered_q5 = aqi_df[(aqi_df['area'] == 'Bengaluru') & (aqi_df['date'] >= start_q5) & (aqi_df['date'] <= end_q5)]\n",
    "category_counts = filtered_q5['air_quality_status'].value_counts()\n",
    "print(\"Q5 Bengaluru Category Counts:\\n\", category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69715ac3-742d-4807-b4da-dba17a94fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6 Trends (sample):\n",
      " {'Agartala': {'slope': 0.03893017918016549, 'persistent': True}, 'Agra': {'slope': -0.0014232614889237943, 'persistent': False}, 'Ahmedabad': {'slope': 0.011129704812530151, 'persistent': True}, 'Ahmednagar': {'slope': 0.19210841932724798, 'persistent': True}, 'Aizawl': {'slope': 0.013389226699217343, 'persistent': False}}\n",
      "Q6 Unhealthy Days (Top 10):\n",
      " area\n",
      "Greater Noida    750\n",
      "Delhi            730\n",
      "Gurugram         724\n",
      "Bhiwadi          659\n",
      "Ghaziabad        657\n",
      "NOIDA            643\n",
      "Faridabad        622\n",
      "Patna            618\n",
      "Muzaffarnagar    571\n",
      "Hajipur          560\n",
      "Name: unhealthy, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Severity Mapping - cities with persistent or worsening AQI, count unhealthy+ days\n",
    "from scipy.stats import linregress\n",
    "from datetime import datetime\n",
    "aqi_df['unhealthy'] = aqi_df['aqi_value'] >= 151\n",
    "unhealthy_days = aqi_df.groupby('area')['unhealthy'].sum()\n",
    "trends = {}\n",
    "for area, group in aqi_df.groupby('area'):\n",
    "    if len(group) < 10: continue\n",
    "    group = group.sort_values('date')\n",
    "    x = (group['date'] - group['date'].min()).dt.days.values\n",
    "    y = group['aqi_value'].values\n",
    "    slope, _, _, _, _ = linregress(x, y)\n",
    "    trends[area] = {'slope': slope, 'persistent': group['aqi_value'].mean() > 100}\n",
    "print(\"Q6 Trends (sample):\\n\", dict(list(trends.items())[:5]))\n",
    "print(\"Q6 Unhealthy Days (Top 10):\\n\", unhealthy_days.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e705c181-9b3a-4b29-b334-eb3a5fdde418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7 Overall Correlation: -0.14838313305561446\n",
      "Q7 State Correlations:\n",
      " state\n",
      "Andhra Pradesh      -0.309552\n",
      "Arunachal Pradesh   -0.244330\n",
      "Assam               -0.216590\n",
      "Bihar               -0.538336\n",
      "Chhattisgarh        -0.471196\n",
      "Gujarat             -0.217269\n",
      "Haryana             -0.130042\n",
      "Himachal Pradesh    -0.075137\n",
      "Jammu and Kashmir   -0.638977\n",
      "Jharkhand           -0.169634\n",
      "Karnataka           -0.153381\n",
      "Kerala               0.242371\n",
      "Madhya Pradesh      -0.473881\n",
      "Maharashtra         -0.088981\n",
      "Manipur              0.066144\n",
      "Meghalaya           -0.331755\n",
      "Mizoram              0.108097\n",
      "Nagaland            -0.279779\n",
      "Odisha              -0.297676\n",
      "Puducherry          -0.288905\n",
      "Punjab              -0.076286\n",
      "Rajasthan            0.020170\n",
      "Sikkim              -1.000000\n",
      "Tamil Nadu          -0.226025\n",
      "Telangana           -0.217908\n",
      "Tripura             -0.017199\n",
      "Uttar Pradesh       -0.355552\n",
      "Uttarakhand          0.003424\n",
      "West Bengal          0.020061\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Health Impact Correlation - correlate AQI spikes with health events\n",
    "air_related = ['Chickenpox', 'Fever with Rash', 'Measles', 'Acute Diarrheal Disease']\n",
    "idsp_filtered = idsp_df[idsp_df['disease_illness_name'].isin(air_related)]\n",
    "health_monthly = idsp_filtered.groupby(['state', 'year', 'month'])['cases'].sum().reset_index(name='health_events')\n",
    "aqi_monthly = aqi_df.groupby(['state', 'year', 'month'])['aqi_value'].agg(['mean', lambda x: (x > 200).sum()]).reset_index()\n",
    "aqi_monthly.columns = ['state', 'year', 'month', 'avg_aqi', 'spikes']\n",
    "merged = pd.merge(aqi_monthly, health_monthly, on=['state', 'year', 'month'], how='inner')\n",
    "# Filter groups with enough data for correlation (at least 2 rows and non-zero variance)\n",
    "merged = merged.groupby('state').filter(lambda g: len(g) >= 2 and g['avg_aqi'].var() > 0 and g['health_events'].var() > 0)\n",
    "overall_corr = merged['avg_aqi'].corr(merged['health_events']) if not merged.empty else np.nan\n",
    "state_corrs = merged.groupby('state').apply(\n",
    "    lambda g: g[['avg_aqi', 'health_events']].corr().iloc[0, 1], include_groups=False\n",
    ").dropna()\n",
    "print(\"Q7 Overall Correlation:\", overall_corr)\n",
    "print(\"Q7 State Correlations:\\n\", state_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c53a632-08da-44cf-a98f-6d45941dde2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8 Spike-Growth Correlation: nan\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Demand Triggers - temporal relationship AQI spikes and vehicle growth\n",
    "vahan_monthly = vahan_df.groupby(['state', 'year', 'month'])['value'].sum().reset_index(name='registrations')\n",
    "vahan_monthly['growth'] = vahan_monthly.groupby('state')['registrations'].pct_change().fillna(0)\n",
    "merged_q8 = pd.merge(aqi_monthly, vahan_monthly, on=['state', 'year', 'month'], how='inner')\n",
    "spike_growth_corr = merged_q8['spikes'].corr(merged_q8['growth'])\n",
    "print(\"Q8 Spike-Growth Correlation:\", spike_growth_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a9e9d3d-d70f-4313-85d3-618639ad1be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q9 Top States Proxies:\n",
      "              state  total_vehicles  population_thousands  population  \\\n",
      "19     Maharashtra         1028348                189698   189698000   \n",
      "31   Uttar Pradesh         1286661                176247   176247000   \n",
      "33     West Bengal          390232                113481   113481000   \n",
      "18  Madhya Pradesh          526548                 78047    78047000   \n",
      "27       Rajasthan          473533                 67110    67110000   \n",
      "7            Delhi          243260                 66635    66635000   \n",
      "14       Karnataka          600589                 93769    93769000   \n",
      "9          Gujarat          570097                109932   109932000   \n",
      "29      Tamil Nadu          695868                127022   127022000   \n",
      "4            Bihar          443359                 48968    48968000   \n",
      "\n",
      "    per_capita_vehicles  high_aqi_days  aqi_burden    households  \\\n",
      "19             0.005421          101.0   19159.498  4.215511e+07   \n",
      "31             0.007300          104.0   18329.688  3.916600e+07   \n",
      "33             0.003439           93.0   10553.733  2.521800e+07   \n",
      "18             0.006747          107.0    8351.029  1.734378e+07   \n",
      "27             0.007056          117.0    7851.870  1.491333e+07   \n",
      "7              0.003651          102.0    6796.770  1.480778e+07   \n",
      "14             0.006405           71.0    6657.599  2.083756e+07   \n",
      "9              0.005186           59.0    6485.988  2.442933e+07   \n",
      "29             0.005478           47.0    5970.034  2.822711e+07   \n",
      "4              0.009054          113.0    5533.384  1.088178e+07   \n",
      "\n",
      "    vehicles_per_hh  \n",
      "19         0.024394  \n",
      "31         0.032851  \n",
      "33         0.015474  \n",
      "18         0.030359  \n",
      "27         0.031752  \n",
      "7          0.016428  \n",
      "14         0.028822  \n",
      "9          0.023337  \n",
      "29         0.024652  \n",
      "4          0.040743  \n"
     ]
    }
   ],
   "source": [
    "# Question 9: Market size proxies using vahan and popu for top states (2025)\n",
    "popu_2025 = popu_df[(popu_df['year'] == 2025) & (popu_df['gender'] == 'Total')].groupby('state')['value'].sum().reset_index(name='population_thousands')\n",
    "popu_2025['population'] = popu_2025['population_thousands'] * 1000\n",
    "vahan_2025 = vahan_df[vahan_df['year'] == 2025].groupby('state')['value'].sum().reset_index(name='total_vehicles')\n",
    "merged_q9 = pd.merge(vahan_2025, popu_2025, on='state', how='inner')\n",
    "merged_q9['per_capita_vehicles'] = merged_q9['total_vehicles'] / merged_q9['population']\n",
    "aqi_2025 = aqi_df[aqi_df['year'] == 2025]\n",
    "high_days = aqi_2025[aqi_2025['aqi_value'] > 150].groupby('state')['date'].nunique().reset_index(name='high_aqi_days')\n",
    "merged_q9 = pd.merge(merged_q9, high_days, on='state', how='left').fillna(0)\n",
    "merged_q9['aqi_burden'] = merged_q9['high_aqi_days'] * merged_q9['population'] / 1e6\n",
    "merged_q9['households'] = merged_q9['population'] / 4.5\n",
    "merged_q9['vehicles_per_hh'] = merged_q9['total_vehicles'] / merged_q9['households']\n",
    "top_states_q9 = merged_q9.sort_values('aqi_burden', ascending=False).head(10)\n",
    "print(\"Q9 Top States Proxies:\\n\", top_states_q9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ee2ed-f827-4019-8824-5ef1a189a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
